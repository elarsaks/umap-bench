{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1ae6f9",
   "metadata": {},
   "source": [
    "# UMAP Performance Analysis: By Metrics\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "**RQ1**: What is the performance impact of individual WASM features (Distance, Tree, Matrix, NN Descent, Optimizer) compared to pure JavaScript?\n",
    "\n",
    "**RQ2**: How does enabling all WASM features together compare to individual features and pure JavaScript?\n",
    "\n",
    "## Methodology\n",
    "\n",
    "- **Test Environment**: All benchmarks run on WSL2 (Windows Subsystem for Linux)\n",
    "- **Baseline**: Pure JavaScript UMAP implementation (no WASM)\n",
    "- **Individual Features**: Each WASM feature enabled separately (Dist, Tree, Matrix, NN, Opt)\n",
    "- **All Features**: All WASM features enabled simultaneously\n",
    "- **Metrics**: Runtime (ms), Memory (MB), Quality (trustworthiness), FPS, Responsiveness (ms)\n",
    "- **Statistical Analysis**: Mann-Whitney U tests, bootstrap confidence intervals, effect sizes\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This notebook is organized by **metrics** to support thesis chapter writing:\n",
    "\n",
    "1. **Setup & Data Preparation**\n",
    "2. **Overview** - Quick summary of all metrics\n",
    "3. **Runtime Performance & Speedup** - Execution time analysis\n",
    "4. **Memory Usage** - Memory consumption patterns\n",
    "5. **Embedding Quality** - Trustworthiness and accuracy\n",
    "6. **Responsiveness** - FPS and interaction latency\n",
    "7. **Dataset Size Effects** - How metrics scale with data size\n",
    "8. **Statistical Significance** - Rigorous hypothesis testing\n",
    "9. **Overall Rankings** - Composite performance scores\n",
    "10. **Export Results** - Save tables and figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbd912",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7972d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, bootstrap\n",
    "\n",
    "# Set styling for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All dependencies loaded successfully\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../outputs/preprocessed.csv')\n",
    "\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nSample of dataset_name and dataset_size:\")\n",
    "print(df[['dataset_name', 'dataset_size']].drop_duplicates().sort_values('dataset_name'))\n",
    "print(f\"\\nDataset size statistics:\")\n",
    "print(df['dataset_size'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bab06d",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d17938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names and prepare data\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Create standardized feature column\n",
    "if 'wasm_features' in df_clean.columns:\n",
    "    df_clean['feature'] = df_clean['wasm_features'].fillna('none').str.lower().str.strip()\n",
    "else:\n",
    "    df_clean['feature'] = df_clean.get('feature', 'none')\n",
    "\n",
    "# Map comma-separated features to 'all'\n",
    "df_clean.loc[df_clean['feature'].astype(str).str.contains(',', na=False), 'feature'] = 'all'\n",
    "\n",
    "# Standardize feature names\n",
    "feature_map = {\n",
    "    'none': 'Baseline (JS)',\n",
    "    'dist': 'Distance',\n",
    "    'tree': 'Tree',\n",
    "    'matrix': 'Matrix',\n",
    "    'nn': 'NN Descent',\n",
    "    'opt': 'Optimizer',\n",
    "    'all': 'All Features'\n",
    "}\n",
    "df_clean['feature_name'] = df_clean['feature'].map(feature_map).fillna(df_clean['feature'])\n",
    "\n",
    "# Identify machine types\n",
    "if 'machine_platform' in df_clean.columns:\n",
    "    platform_map = {'darwin': 'MacBook', 'linux': 'Linux'}\n",
    "    df_clean['machine_type'] = df_clean['machine_platform'].map(platform_map).fillna(df_clean['machine_platform'])\n",
    "else:\n",
    "    if 'machine_type' not in df_clean.columns:\n",
    "        df_clean['machine_type'] = pd.NA\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['runtime_ms', 'memory_delta_mb', 'trustworthiness', 'fps_avg', 'responsiveness_ms']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "# Remove missing values in key columns\n",
    "before = len(df_clean)\n",
    "required = [c for c in ['runtime_ms', 'feature_name'] if c in df_clean.columns]\n",
    "df_clean = df_clean.dropna(subset=required)\n",
    "dropped = before - len(df_clean)\n",
    "\n",
    "# Create analysis dataset\n",
    "df_analysis = df_clean.copy()\n",
    "\n",
    "# Define standard feature order\n",
    "feature_order = ['Baseline (JS)', 'Distance', 'Tree', 'Matrix', 'NN Descent', 'Optimizer', 'All Features']\n",
    "feature_order = [f for f in feature_order if f in df_analysis['feature_name'].unique()]\n",
    "\n",
    "print(f\"✓ Dataset ready: {len(df_analysis):,} measurements (dropped {dropped} rows)\")\n",
    "print(f\"Features: {sorted(df_analysis['feature_name'].unique())}\")\n",
    "print(f\"Datasets: {df_analysis['dataset_name'].nunique()}\")\n",
    "print(f\"\\nMeasurements per feature:\")\n",
    "print(df_analysis['feature_name'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1a092",
   "metadata": {},
   "source": [
    "## 2. Overview: All Metrics Summary\n",
    "\n",
    "Quick overview of all performance metrics across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics for all metrics\n",
    "baseline_label = 'Baseline (JS)'\n",
    "\n",
    "# Calculate medians for each metric by feature\n",
    "summary_stats = df_analysis.groupby('feature_name').agg({\n",
    "    'runtime_ms': ['median', 'mean', 'std'],\n",
    "    'memory_delta_mb': ['median', 'mean', 'std'],\n",
    "    'trustworthiness': ['median', 'mean', 'std'],\n",
    "    'fps_avg': ['median', 'mean', 'std'],\n",
    "    'responsiveness_ms': ['median', 'mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"Summary Statistics by Feature (ordered):\")\n",
    "display(summary_stats.loc[feature_order])\n",
    "\n",
    "# Calculate speedups\n",
    "runtime_medians = df_analysis.groupby('feature_name')['runtime_ms'].median()\n",
    "speedup_rows = []\n",
    "if baseline_label in runtime_medians.index:\n",
    "    baseline_rt = runtime_medians[baseline_label]\n",
    "    for feat, rt in runtime_medians.drop(baseline_label).items():\n",
    "        if rt > 0:\n",
    "            speedup_rows.append({'feature': feat, 'speedup': baseline_rt / rt, 'improvement_%': ((baseline_rt / rt) - 1) * 100})\n",
    "\n",
    "speedup_summary = pd.DataFrame(speedup_rows).sort_values('speedup', ascending=False)\n",
    "print(\"\\nSpeedup vs Baseline:\")\n",
    "display(speedup_summary.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c32a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview visualization: 4-panel metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Runtime\n",
    "sns.boxplot(data=df_analysis, x='feature_name', y='runtime_ms', order=feature_order,\n",
    "            ax=axes[0, 0], showfliers=False, palette='Blues')\n",
    "axes[0, 0].set_title('Runtime Performance', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('')\n",
    "axes[0, 0].set_ylabel('Runtime (ms)', fontsize=11)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Quality\n",
    "if 'trustworthiness' in df_analysis:\n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='trustworthiness', order=feature_order,\n",
    "                ax=axes[0, 1], showfliers=False, palette='Greens')\n",
    "    axes[0, 1].set_title('Embedding Quality', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('')\n",
    "    axes[0, 1].set_ylabel('Trustworthiness', fontsize=11)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# FPS\n",
    "if 'fps_avg' in df_analysis:\n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='fps_avg', order=feature_order,\n",
    "                ax=axes[1, 0], showfliers=False, palette='Purples')\n",
    "    axes[1, 0].set_title('Responsiveness (FPS)', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('WASM Feature', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('FPS', fontsize=11)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Memory\n",
    "if 'memory_delta_mb' in df_analysis:\n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='memory_delta_mb', order=feature_order,\n",
    "                ax=axes[1, 1], showfliers=False, palette='Oranges')\n",
    "    axes[1, 1].set_title('Memory Usage', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('WASM Feature', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Memory Delta (MB)', fontsize=11)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/overview_all_metrics.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544cc67",
   "metadata": {},
   "source": [
    "## 3. Runtime Performance & Speedup\n",
    "\n",
    "Detailed analysis of execution time and speedup relative to baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5030b4",
   "metadata": {},
   "source": [
    "### 3.1 Runtime Distribution by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba66e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime statistics by feature\n",
    "runtime_stats = df_analysis.groupby('feature_name')['runtime_ms'].describe()\n",
    "print(\"Runtime Statistics (ms):\")\n",
    "display(runtime_stats.loc[feature_order].round(2))\n",
    "\n",
    "# Baseline metrics\n",
    "if baseline_label in df_analysis['feature_name'].values:\n",
    "    baseline_rt = df_analysis[df_analysis['feature_name'] == baseline_label]['runtime_ms']\n",
    "    print(f\"\\nBaseline (Pure JavaScript):\") \n",
    "    print(f\"  Median: {baseline_rt.median():.2f} ms\")\n",
    "    print(f\"  Mean: {baseline_rt.mean():.2f} ms (±{baseline_rt.std():.2f})\")\n",
    "    print(f\"  Range: {baseline_rt.min():.2f} - {baseline_rt.max():.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85153471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime distribution visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.boxplot(data=df_analysis, x='feature_name', y='runtime_ms', order=feature_order, \n",
    "            ax=ax, showfliers=False, palette='Blues')\n",
    "ax.set_title('Runtime Distribution by WASM Feature', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('WASM Feature', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Runtime (ms)', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/runtime_distribution.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad6e39",
   "metadata": {},
   "source": [
    "### 3.2 Speedup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a196c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed speedup metrics\n",
    "def calculate_speedup(df, baseline='Baseline (JS)'):\n",
    "    results = []\n",
    "    data = df\n",
    "    \n",
    "    for (dataset, mach), group in data.groupby(['dataset_name', 'machine_type']):\n",
    "        baseline_data = group[group['feature_name'] == baseline]['runtime_ms']\n",
    "        if len(baseline_data) == 0:\n",
    "            continue\n",
    "        baseline_median = baseline_data.median()\n",
    "        \n",
    "        for feature in group['feature_name'].unique():\n",
    "            if feature == baseline:\n",
    "                continue\n",
    "            feature_data = group[group['feature_name'] == feature]\n",
    "            if len(feature_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            feature_median = feature_data['runtime_ms'].median()\n",
    "            speedup = baseline_median / feature_median\n",
    "            \n",
    "            results.append({\n",
    "                'dataset': dataset,\n",
    "                'machine': mach,\n",
    "                'feature': feature,\n",
    "                'baseline_median_ms': baseline_median,\n",
    "                'feature_median_ms': feature_median,\n",
    "                'speedup': speedup,\n",
    "                'improvement_pct': (speedup - 1) * 100\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "speedup_df = calculate_speedup(df_analysis)\n",
    "\n",
    "# Aggregate speedup statistics\n",
    "speedup_summary = speedup_df.groupby('feature').agg({\n",
    "    'speedup': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'improvement_pct': ['mean', 'median']\n",
    "}).round(3)\n",
    "\n",
    "print(\"Speedup Summary (vs Baseline):\")\n",
    "display(speedup_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2711987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speedup visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculate median speedup for each feature\n",
    "feature_speedups = speedup_df.groupby('feature')['speedup'].median().sort_values(ascending=False)\n",
    "colors = ['green' if x > 1 else 'red' for x in feature_speedups.values]\n",
    "\n",
    "bars = ax.barh(feature_speedups.index, feature_speedups.values, color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2., f'{width:.2f}x',\n",
    "           ha='left', va='center', fontsize=10, fontweight='bold',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax.axvline(x=1.0, color='black', linestyle='--', linewidth=2, label='Baseline (1.0x)', alpha=0.7)\n",
    "ax.set_xlabel('Speedup (vs Pure JavaScript)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Median Speedup by WASM Feature', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/speedup_analysis.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  >1.0x = Faster than baseline (performance improvement)\")\n",
    "print(\"  <1.0x = Slower than baseline (performance regression)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c3540",
   "metadata": {},
   "source": [
    "## 4. Memory Usage\n",
    "\n",
    "Analysis of memory consumption patterns across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff783379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory statistics by feature\n",
    "if 'memory_delta_mb' in df_analysis:\n",
    "    memory_stats = df_analysis.groupby('feature_name')['memory_delta_mb'].describe()\n",
    "    print(\"Memory Usage Statistics (MB):\")\n",
    "    display(memory_stats.loc[feature_order].round(2))\n",
    "    \n",
    "    # Baseline comparison\n",
    "    if baseline_label in df_analysis['feature_name'].values:\n",
    "        baseline_mem = df_analysis[df_analysis['feature_name'] == baseline_label]['memory_delta_mb']\n",
    "        print(f\"\\nBaseline Memory Usage:\")\n",
    "        print(f\"  Median: {baseline_mem.median():.2f} MB\")\n",
    "        print(f\"  Mean: {baseline_mem.mean():.2f} MB (±{baseline_mem.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb69b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage visualization\n",
    "if 'memory_delta_mb' in df_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='memory_delta_mb', order=feature_order,\n",
    "                ax=ax, showfliers=False, palette='Oranges')\n",
    "    ax.set_title('Memory Usage by WASM Feature', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('WASM Feature', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Memory Delta (MB)', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/memory_usage.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36168476",
   "metadata": {},
   "source": [
    "## 5. Embedding Quality (Trustworthiness)\n",
    "\n",
    "Analysis of UMAP embedding quality across features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality statistics by feature\n",
    "if 'trustworthiness' in df_analysis:\n",
    "    quality_stats = df_analysis.groupby('feature_name')['trustworthiness'].describe()\n",
    "    print(\"Embedding Quality (Trustworthiness):\")\n",
    "    display(quality_stats.loc[feature_order].round(4))\n",
    "    \n",
    "    # Check if quality is preserved\n",
    "    baseline_quality = df_analysis[df_analysis['feature_name'] == baseline_label]['trustworthiness'].median()\n",
    "    print(f\"\\nBaseline Quality: {baseline_quality:.4f}\")\n",
    "    \n",
    "    for feat in feature_order:\n",
    "        if feat == baseline_label:\n",
    "            continue\n",
    "        feat_quality = df_analysis[df_analysis['feature_name'] == feat]['trustworthiness'].median()\n",
    "        diff = feat_quality - baseline_quality\n",
    "        pct_diff = (diff / baseline_quality) * 100\n",
    "        status = \"✓\" if abs(pct_diff) < 1 else (\"↑\" if diff > 0 else \"↓\")\n",
    "        print(f\"  {feat}: {feat_quality:.4f} ({pct_diff:+.2f}%) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality distribution visualization\n",
    "if 'trustworthiness' in df_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='trustworthiness', order=feature_order,\n",
    "                ax=ax, showfliers=False, palette='Greens')\n",
    "    ax.set_title('Embedding Quality by WASM Feature', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('WASM Feature', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Trustworthiness', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/quality_analysis.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34be4aa",
   "metadata": {},
   "source": [
    "### Quality vs Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d153af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality vs runtime scatter plot\n",
    "if 'trustworthiness' in df_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    for feature in feature_order:\n",
    "        feature_data = df_analysis[df_analysis['feature_name'] == feature]\n",
    "        ax.scatter(feature_data['runtime_ms'], feature_data['trustworthiness'],\n",
    "                  label=feature, alpha=0.6, s=100)\n",
    "    \n",
    "    ax.set_xlabel('Runtime (ms)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Trustworthiness', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Quality vs Performance Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='best', framealpha=0.9)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/quality_vs_performance.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Interpretation: Points closer to the bottom-right indicate better performance with maintained quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde71c7f",
   "metadata": {},
   "source": [
    "## 6. Responsiveness: FPS & Interaction Latency\n",
    "\n",
    "Analysis of user-facing performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1e791",
   "metadata": {},
   "source": [
    "### 6.1 Frames Per Second (FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPS statistics by feature\n",
    "if 'fps_avg' in df_analysis:\n",
    "    fps_stats = df_analysis.groupby('feature_name')['fps_avg'].describe()\n",
    "    print(\"FPS Statistics:\")\n",
    "    display(fps_stats.loc[feature_order].round(2))\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_fps = df_analysis[df_analysis['feature_name'] == baseline_label]['fps_avg'].median()\n",
    "    print(f\"\\nBaseline FPS: {baseline_fps:.2f}\")\n",
    "    \n",
    "    for feat in feature_order:\n",
    "        if feat == baseline_label:\n",
    "            continue\n",
    "        feat_fps = df_analysis[df_analysis['feature_name'] == feat]['fps_avg'].median()\n",
    "        diff = feat_fps - baseline_fps\n",
    "        pct_diff = (diff / baseline_fps) * 100\n",
    "        print(f\"  {feat}: {feat_fps:.2f} FPS ({pct_diff:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ff885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPS visualization\n",
    "if 'fps_avg' in df_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='fps_avg', order=feature_order,\n",
    "                ax=ax, showfliers=False, palette='Purples')\n",
    "    ax.set_title('FPS Distribution by WASM Feature', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('WASM Feature', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('FPS (avg)', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add 60 FPS reference line\n",
    "    ax.axhline(y=60, color='green', linestyle='--', linewidth=2, alpha=0.7, label='60 FPS target')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/fps_analysis.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cac8ad",
   "metadata": {},
   "source": [
    "### 6.2 Interaction Latency (Responsiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875026ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsiveness statistics by feature\n",
    "if 'responsiveness_ms' in df_analysis:\n",
    "    resp_stats = df_analysis.groupby('feature_name')['responsiveness_ms'].describe()\n",
    "    print(\"Responsiveness Statistics (ms):\")\n",
    "    display(resp_stats.loc[feature_order].round(2))\n",
    "    \n",
    "    # Baseline comparison\n",
    "    baseline_resp = df_analysis[df_analysis['feature_name'] == baseline_label]['responsiveness_ms'].median()\n",
    "    print(f\"\\nBaseline Responsiveness: {baseline_resp:.2f} ms\")\n",
    "    \n",
    "    for feat in feature_order:\n",
    "        if feat == baseline_label:\n",
    "            continue\n",
    "        feat_resp = df_analysis[df_analysis['feature_name'] == feat]['responsiveness_ms'].median()\n",
    "        diff = feat_resp - baseline_resp\n",
    "        pct_diff = (diff / baseline_resp) * 100\n",
    "        status = \"✓\" if diff < 0 else \"↑\"\n",
    "        print(f\"  {feat}: {feat_resp:.2f} ms ({pct_diff:+.2f}%) {status}\")\n",
    "    \n",
    "    print(\"\\nNote: Lower responsiveness = Better (less latency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b43c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsiveness visualization\n",
    "if 'responsiveness_ms' in df_analysis:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=df_analysis, x='feature_name', y='responsiveness_ms', order=feature_order,\n",
    "                ax=ax, showfliers=False, palette='Reds_r')\n",
    "    ax.set_title('Interaction Latency by WASM Feature', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('WASM Feature', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Responsiveness (ms)', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/responsiveness_analysis.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c17daf",
   "metadata": {},
   "source": [
    "## 7. Dataset Size Effects\n",
    "\n",
    "How each metric scales with dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset size analysis\n",
    "df_analysis['dataset_size'] = pd.to_numeric(df_analysis['dataset_size'], errors='coerce')\n",
    "\n",
    "# Create size categories\n",
    "df_analysis['size_category'] = pd.cut(\n",
    "    df_analysis['dataset_size'],\n",
    "    bins=[0, 200, 800, float('inf')],\n",
    "    labels=['Small (≤200)', 'Medium (200-800)', 'Large (>800)']\n",
    ")\n",
    "\n",
    "print(\"Dataset Size Distribution:\")\n",
    "print(df_analysis.groupby('dataset_name')['dataset_size'].first().sort_values())\n",
    "print(f\"\\nSize category counts:\")\n",
    "print(df_analysis['size_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea1eaa",
   "metadata": {},
   "source": [
    "### Runtime Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd76dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime scaling with dataset size\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for feature in feature_order:\n",
    "    feature_data = df_analysis[df_analysis['feature_name'] == feature]\n",
    "    if len(feature_data) == 0:\n",
    "        continue\n",
    "    size_runtime = feature_data.groupby('dataset_size')['runtime_ms'].median().sort_index()\n",
    "    ax.plot(size_runtime.index, size_runtime.values, marker='o', label=feature, \n",
    "           linewidth=2, markersize=10, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Dataset Size (samples)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Median Runtime (ms)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Runtime Scaling by Dataset Size', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/runtime_scaling.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec445fa",
   "metadata": {},
   "source": [
    "### Speedup by Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d158c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup by size\n",
    "def calculate_speedup_by_size(df, baseline='Baseline (JS)'):\n",
    "    results = []\n",
    "    for (size, machine), group in df.groupby(['dataset_size', 'machine_type']):\n",
    "        baseline_data = group[group['feature_name'] == baseline]['runtime_ms']\n",
    "        if len(baseline_data) == 0:\n",
    "            continue\n",
    "        baseline_median = baseline_data.median()\n",
    "        \n",
    "        for feature in group['feature_name'].unique():\n",
    "            if feature == baseline:\n",
    "                continue\n",
    "            feature_data = group[group['feature_name'] == feature]\n",
    "            if len(feature_data) == 0:\n",
    "                continue\n",
    "            feature_median = feature_data['runtime_ms'].median()\n",
    "            speedup = baseline_median / feature_median\n",
    "            \n",
    "            results.append({\n",
    "                'dataset_size': size,\n",
    "                'feature': feature,\n",
    "                'speedup': speedup\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "speedup_by_size = calculate_speedup_by_size(df_analysis)\n",
    "\n",
    "# Visualize speedup trends\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for feature in speedup_by_size['feature'].unique():\n",
    "    feat_data = speedup_by_size[speedup_by_size['feature'] == feature].sort_values('dataset_size')\n",
    "    if len(feat_data) == 0:\n",
    "        continue\n",
    "    ax.plot(feat_data['dataset_size'], feat_data['speedup'], \n",
    "           marker='o', label=feature, linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Baseline (1.0x)')\n",
    "ax.set_xlabel('Dataset Size (samples)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Speedup vs Baseline', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Speedup Trend by Dataset Size', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/speedup_by_size.png', bbox_inches='tight', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18190d",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Testing\n",
    "\n",
    "Rigorous statistical tests comparing each feature to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ff445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U test\n",
    "def mann_whitney_test(df, baseline='Baseline (JS)'):\n",
    "    results = []\n",
    "    baseline_data = df[df['feature_name'] == baseline]['runtime_ms'].dropna()\n",
    "    \n",
    "    if len(baseline_data) < 3:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for feature in df['feature_name'].unique():\n",
    "        if feature == baseline:\n",
    "            continue\n",
    "        feature_data = df[df['feature_name'] == feature]['runtime_ms'].dropna()\n",
    "        if len(feature_data) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        statistic, p_value = mannwhitneyu(baseline_data, feature_data, alternative='two-sided')\n",
    "        \n",
    "        # Cliff's Delta (effect size)\n",
    "        n1, n2 = len(baseline_data), len(feature_data)\n",
    "        pairs_greater = sum(1 for b in baseline_data for f in feature_data if b > f)\n",
    "        pairs_less = sum(1 for b in baseline_data for f in feature_data if b < f)\n",
    "        cliffs_delta = (pairs_greater - pairs_less) / (n1 * n2)\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        abs_delta = abs(cliffs_delta)\n",
    "        if abs_delta < 0.147:\n",
    "            effect_size = 'negligible'\n",
    "        elif abs_delta < 0.33:\n",
    "            effect_size = 'small'\n",
    "        elif abs_delta < 0.474:\n",
    "            effect_size = 'medium'\n",
    "        else:\n",
    "            effect_size = 'large'\n",
    "        \n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'n_baseline': len(baseline_data),\n",
    "            'n_feature': len(feature_data),\n",
    "            'baseline_median': baseline_data.median(),\n",
    "            'feature_median': feature_data.median(),\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'cliffs_delta': cliffs_delta,\n",
    "            'effect_size': effect_size\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "test_results = mann_whitney_test(df_analysis)\n",
    "\n",
    "print(\"Statistical Test Results (Mann-Whitney U Test):\")\n",
    "print(\"\\nSignificance: p < 0.05\")\n",
    "print(\"Cliff's Delta: negative = feature is faster\\n\")\n",
    "display(test_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect sizes\n",
    "if len(test_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    test_results_sorted = test_results.sort_values('cliffs_delta')\n",
    "    colors = ['green' if sig else 'gray' for sig in test_results_sorted['significant']]\n",
    "    \n",
    "    ax.barh(test_results_sorted['feature'], test_results_sorted['cliffs_delta'], \n",
    "           color=colors, alpha=0.7)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax.axvline(x=-0.147, color='orange', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.axvline(x=0.147, color='orange', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.set_xlabel(\"Cliff's Delta (Effect Size)\", fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Effect Sizes (Green = p<0.05, Gray = n.s.)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/statistical_significance.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Negative = feature faster than baseline\")\n",
    "    print(\"Positive = feature slower than baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee90590",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041238a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap CI for speedup\n",
    "def bootstrap_speedup_ci(df, baseline='Baseline (JS)', n_bootstrap=10000, confidence=0.95):\n",
    "    results = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    baseline_data = df[df['feature_name'] == baseline]['runtime_ms'].dropna().values\n",
    "    \n",
    "    if len(baseline_data) < 3:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for feature in df['feature_name'].unique():\n",
    "        if feature == baseline:\n",
    "            continue\n",
    "        feature_data = df[df['feature_name'] == feature]['runtime_ms'].dropna().values\n",
    "        if len(feature_data) < 3:\n",
    "            continue\n",
    "        \n",
    "        speedups = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            base_sample = rng.choice(baseline_data, size=len(baseline_data), replace=True)\n",
    "            feat_sample = rng.choice(feature_data, size=len(feature_data), replace=True)\n",
    "            speedup = np.median(base_sample) / np.median(feat_sample)\n",
    "            speedups.append(speedup)\n",
    "        \n",
    "        speedups = np.array(speedups)\n",
    "        alpha = 1 - confidence\n",
    "        ci_lower = np.percentile(speedups, alpha/2 * 100)\n",
    "        ci_upper = np.percentile(speedups, (1 - alpha/2) * 100)\n",
    "        \n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'speedup_median': np.median(speedups),\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'ci_width': ci_upper - ci_lower\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Calculating bootstrap confidence intervals (10,000 iterations)...\")\n",
    "bootstrap_results = bootstrap_speedup_ci(df_analysis)\n",
    "\n",
    "print(\"\\n95% Confidence Intervals for Speedup:\")\n",
    "display(bootstrap_results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence intervals\n",
    "if len(bootstrap_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    bootstrap_results_sorted = bootstrap_results.sort_values('speedup_median')\n",
    "    y_pos = np.arange(len(bootstrap_results_sorted))\n",
    "    \n",
    "    ax.errorbar(\n",
    "        bootstrap_results_sorted['speedup_median'],\n",
    "        y_pos,\n",
    "        xerr=[\n",
    "            bootstrap_results_sorted['speedup_median'] - bootstrap_results_sorted['ci_lower'],\n",
    "            bootstrap_results_sorted['ci_upper'] - bootstrap_results_sorted['speedup_median']\n",
    "        ],\n",
    "        fmt='o', markersize=10, capsize=5, capthick=2, elinewidth=2, color='steelblue'\n",
    "    )\n",
    "    \n",
    "    ax.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Baseline')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(bootstrap_results_sorted['feature'])\n",
    "    ax.set_xlabel('Speedup (95% CI)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Speedup with Confidence Intervals', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/bootstrap_confidence_intervals.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b8397",
   "metadata": {},
   "source": [
    "## 9. Overall Rankings: Composite Performance Scores\n",
    "\n",
    "Rank features using a weighted composite score across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate composite performance scores\n",
    "def calculate_composite_scores(df, baseline='Baseline (JS)'):\n",
    "    results = []\n",
    "    \n",
    "    for feature in df['feature_name'].unique():\n",
    "        if feature == baseline:\n",
    "            continue\n",
    "        \n",
    "        feature_data = df[df['feature_name'] == feature]\n",
    "        baseline_data = df[df['feature_name'] == baseline]\n",
    "        \n",
    "        # Runtime speedup\n",
    "        speedup = baseline_data['runtime_ms'].median() / feature_data['runtime_ms'].median()\n",
    "        \n",
    "        # Quality ratio\n",
    "        quality_ratio = feature_data['trustworthiness'].median() / baseline_data['trustworthiness'].median() if 'trustworthiness' in df else 1.0\n",
    "        \n",
    "        # FPS ratio\n",
    "        fps_ratio = feature_data['fps_avg'].median() / baseline_data['fps_avg'].median() if 'fps_avg' in df else 1.0\n",
    "        \n",
    "        # Memory impact (lower is better, normalize to 0-1 scale)\n",
    "        memory_delta = feature_data['memory_delta_mb'].median() if 'memory_delta_mb' in df else 0\n",
    "        memory_score = max(0, 1 - abs(memory_delta) / 100)  # Normalize\n",
    "        \n",
    "        # Composite score: weighted average\n",
    "        # Weights: 50% speedup, 25% quality, 15% FPS, 10% memory\n",
    "        composite = (0.50 * speedup + 0.25 * quality_ratio + 0.15 * fps_ratio + 0.10 * memory_score)\n",
    "        \n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'speedup': speedup,\n",
    "            'quality_ratio': quality_ratio,\n",
    "            'fps_ratio': fps_ratio,\n",
    "            'memory_score': memory_score,\n",
    "            'composite_score': composite\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('composite_score', ascending=False)\n",
    "\n",
    "rankings = calculate_composite_scores(df_analysis)\n",
    "\n",
    "print(\"Overall Performance Rankings:\")\n",
    "print(\"=\"*80)\n",
    "display(rankings.round(3))\n",
    "\n",
    "print(\"\\nTop 3 Features:\")\n",
    "for i, (idx, row) in enumerate(rankings.head(3).iterrows(), 1):\n",
    "    print(f\"{i}. {row['feature']} (score: {row['composite_score']:.3f})\")\n",
    "    print(f\"   - Speedup: {row['speedup']:.2f}x\")\n",
    "    print(f\"   - Quality ratio: {row['quality_ratio']:.3f}\")\n",
    "    print(f\"   - FPS ratio: {row['fps_ratio']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rankings visualization\n",
    "if len(rankings) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(rankings)))\n",
    "    bars = ax.barh(rankings['feature'], rankings['composite_score'], color=colors)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2., f'{width:.3f}',\n",
    "               ha='left', va='center', fontsize=10, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('Composite Score (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Overall Feature Rankings', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/overall_rankings.png', bbox_inches='tight', dpi=200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca4e09",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Save all analysis results to CSV files for thesis inclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summaries directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../outputs/summaries', exist_ok=True)\n",
    "\n",
    "# Export summary tables\n",
    "if len(speedup_df) > 0:\n",
    "    speedup_df.to_csv('../outputs/summaries/speedup_analysis.csv', index=False)\n",
    "    print(\"✓ Saved speedup_analysis.csv\")\n",
    "\n",
    "if len(test_results) > 0:\n",
    "    test_results.to_csv('../outputs/summaries/statistical_tests.csv', index=False)\n",
    "    print(\"✓ Saved statistical_tests.csv\")\n",
    "\n",
    "if len(bootstrap_results) > 0:\n",
    "    bootstrap_results.to_csv('../outputs/summaries/bootstrap_confidence_intervals.csv', index=False)\n",
    "    print(\"✓ Saved bootstrap_confidence_intervals.csv\")\n",
    "\n",
    "if len(rankings) > 0:\n",
    "    rankings.to_csv('../outputs/summaries/feature_rankings.csv', index=False)\n",
    "    print(\"✓ Saved feature_rankings.csv\")\n",
    "\n",
    "# Export metric-specific summaries\n",
    "summary_stats.to_csv('../outputs/summaries/metrics_summary.csv')\n",
    "print(\"✓ Saved metrics_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All analysis results exported to ../outputs/summaries/\")\n",
    "print(\"All figures saved to ../outputs/figures/\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3bd0d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed UMAP performance across multiple metrics:\n",
    "\n",
    "1. **Runtime & Speedup**: Execution time and performance gains\n",
    "2. **Memory Usage**: Memory consumption patterns\n",
    "3. **Embedding Quality**: Trustworthiness preservation\n",
    "4. **Responsiveness**: FPS and interaction latency\n",
    "5. **Dataset Size Effects**: Scaling behavior\n",
    "6. **Statistical Significance**: Rigorous hypothesis testing\n",
    "7. **Overall Rankings**: Composite performance scores\n",
    "\n",
    "**For thesis writing**: Each section above corresponds to a potential results subsection organized by metric. All figures and tables have been exported for easy inclusion in your thesis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
