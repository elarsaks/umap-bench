{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UMAP Benchmark: Preprocess Results\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook preprocesses UMAP benchmark results by flattening nested JSON files into a single tabular CSV format suitable for statistical analysis. \n",
        "\n",
        "**Key Features:**\n",
        "- Extracts performance metrics from benchmark runs\n",
        "- Combines machine metadata, git information, and UI performance data\n",
        "- Automatically filters out incomplete results (missing UI metrics or empty rows)\n",
        "- Outputs a clean, analysis-ready CSV file\n",
        "\n",
        "**Input:** Multiple JSON files in `../results/` following the pattern `bench-runs-*.json`\n",
        "\n",
        "**Output:** `../outputs/preprocessed.csv` with all benchmark data in tabular format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd59e1c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths relative to bench/analysis/ folder\n",
        "results_dir = \"../results\"\n",
        "output_path = \"../outputs/preprocessed.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd00ae75",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "Import required libraries and configure paths:\n",
        "- `results_dir`: Directory containing benchmark JSON files\n",
        "- `output_path`: Path for the output CSV file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8284c73f",
      "metadata": {},
      "source": [
        "## Data Processing\n",
        "\n",
        "The following cell processes each benchmark JSON file and extracts:\n",
        "\n",
        "1. **Top-level metadata**: Generation time, run count, WASM configuration\n",
        "2. **Machine information**: Platform, CPU, memory, load averages\n",
        "3. **Git context**: Commit hash, branch, dirty status\n",
        "4. **Result metrics**: Run number, duration\n",
        "5. **Performance data**: Runtime, memory usage, trustworthiness, FPS, responsiveness\n",
        "\n",
        "Each metric row (one per dataset) becomes a separate record in the output, with all parent context preserved.\n",
        "\n",
        "Note: Supports both the new flat `metrics[]` format and the legacy `uiMetrics[].rows[]` format for backward compatibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5662f4c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = []\n",
        "files = sorted(glob.glob(os.path.join(results_dir, \"bench-runs-*.json\")))\n",
        "\n",
        "for path in files:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    top = {\n",
        "        \"generated_at\": data.get(\"generatedAt\"),\n",
        "        \"runs_declared\": data.get(\"runs\"),\n",
        "        \"wasm_features_file\": data.get(\"wasmFeatures\"),\n",
        "        \"wasm_preload\": data.get(\"wasmPreload\"),\n",
        "    }\n",
        "\n",
        "    machine = data.get(\"machine\") or {}\n",
        "    top.update({\n",
        "        \"machine_platform\": machine.get(\"platform\"),\n",
        "        \"machine_release\": machine.get(\"release\"),\n",
        "        \"machine_arch\": machine.get(\"arch\"),\n",
        "        \"cpu_model\": machine.get(\"cpuModel\"),\n",
        "        \"cpu_cores\": machine.get(\"cpuCores\"),\n",
        "        \"total_mem_bytes\": machine.get(\"totalMemBytes\"),\n",
        "        \"load_avg_1\": (machine.get(\"loadAvg\") or [None, None, None])[0],\n",
        "        \"load_avg_5\": (machine.get(\"loadAvg\") or [None, None, None])[1],\n",
        "        \"load_avg_15\": (machine.get(\"loadAvg\") or [None, None, None])[2],\n",
        "        \"hostname\": machine.get(\"hostname\"),\n",
        "    })\n",
        "\n",
        "    git = data.get(\"git\") or {}\n",
        "    top.update({\n",
        "        \"git_commit\": git.get(\"commit\"),\n",
        "        \"git_branch\": git.get(\"branch\"),\n",
        "        \"git_status_dirty\": git.get(\"statusDirty\"),\n",
        "    })\n",
        "\n",
        "    for result in data.get(\"results\", []):\n",
        "        # Support both old format (uiMetrics[].rows[]) and new format (metrics[])\n",
        "        metrics_list = result.get(\"metrics\")\n",
        "        if metrics_list is None:\n",
        "            # Old format fallback\n",
        "            ui_metrics = result.get(\"uiMetrics\") or []\n",
        "            metrics_list = []\n",
        "            for ui in ui_metrics:\n",
        "                metrics_list.extend(ui.get(\"rows\") or [])\n",
        "        \n",
        "        if not metrics_list:\n",
        "            continue\n",
        "            \n",
        "        base = dict(top)\n",
        "        base.update({\n",
        "            \"result_run\": result.get(\"run\"),\n",
        "            \"result_duration_ms\": result.get(\"durationMs\"),\n",
        "        })\n",
        "\n",
        "        stats = result.get(\"stats\") or {}\n",
        "        base.update({\n",
        "            \"stats_start_time\": stats.get(\"startTime\"),\n",
        "            \"stats_duration_ms\": stats.get(\"duration\"),\n",
        "        })\n",
        "\n",
        "        for row in metrics_list:\n",
        "            # Do not infer renderingEnabled. If missing, default to False.\n",
        "            rendering_enabled = row.get(\"renderingEnabled\")\n",
        "            if rendering_enabled is None:\n",
        "                rendering_enabled = False\n",
        "            \n",
        "            out = dict(base)\n",
        "            out.update({\n",
        "                \"dataset_index\": row.get(\"datasetIndex\") or row.get(\"runId\"),  # Support old field name\n",
        "                \"timestamp\": row.get(\"timestamp\"),\n",
        "                \"dataset_name\": row.get(\"datasetName\"),\n",
        "                \"dataset_size\": row.get(\"datasetSize\"),\n",
        "                \"dimensions\": row.get(\"dimensions\"),\n",
        "                \"wasm_features\": row.get(\"wasmFeatures\"),\n",
        "                \"rendering_enabled\": rendering_enabled,\n",
        "                \"runtime_ms\": row.get(\"runtimeMs\"),\n",
        "                \"memory_delta_mb\": row.get(\"memoryDeltaMb\"),\n",
        "                \"trustworthiness\": row.get(\"trustworthiness\"),\n",
        "                \"fps_avg\": row.get(\"fpsAvg\"),\n",
        "                \"responsiveness_ms\": row.get(\"responsivenessMs\"),\n",
        "            })\n",
        "            rows.append(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec736c4",
      "metadata": {},
      "source": [
        "## Processing Summary\n",
        "\n",
        "Review the data extraction results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018fb38e",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Files processed: {len(files)}\")\n",
        "print(f\"Total rows extracted: {len(rows)}\")\n",
        "print(f\"\\nSample of first row keys:\")\n",
        "if rows:\n",
        "    for key in list(rows[0].keys())[:10]:\n",
        "        print(f\"  - {key}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb2ca1e",
      "metadata": {},
      "source": [
        "## Create DataFrame and Export\n",
        "\n",
        "Convert the extracted rows into a pandas DataFrame with a consistent column order, then export to CSV. The output file will be ready for statistical analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f99755",
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = [\n",
        "    # File/run metadata\n",
        "    \"generated_at\",\n",
        "    \"runs_declared\",\n",
        "    \"result_run\",\n",
        "    \"result_duration_ms\",\n",
        "    \"stats_start_time\",\n",
        "    \"stats_duration_ms\",\n",
        "    # WASM config (file-level)\n",
        "    \"wasm_features_file\",\n",
        "    \"wasm_preload\",\n",
        "    # Machine info\n",
        "    \"machine_platform\",\n",
        "    \"machine_release\",\n",
        "    \"machine_arch\",\n",
        "    \"cpu_model\",\n",
        "    \"cpu_cores\",\n",
        "    \"total_mem_bytes\",\n",
        "    \"load_avg_1\",\n",
        "    \"load_avg_5\",\n",
        "    \"load_avg_15\",\n",
        "    \"hostname\",\n",
        "    # Git context\n",
        "    \"git_commit\",\n",
        "    \"git_branch\",\n",
        "    \"git_status_dirty\",\n",
        "    # Per-dataset metrics (the core data)\n",
        "    \"dataset_index\",\n",
        "    \"timestamp\",\n",
        "    \"dataset_name\",\n",
        "    \"dataset_size\",\n",
        "    \"dimensions\",\n",
        "    \"wasm_features\",\n",
        "    \"rendering_enabled\",\n",
        "    \"runtime_ms\",\n",
        "    \"memory_delta_mb\",\n",
        "    \"trustworthiness\",\n",
        "    \"fps_avg\",\n",
        "    \"responsiveness_ms\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(rows).reindex(columns=columns)\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"âœ“ Successfully saved {len(df)} rows to '{output_path}'\")\n",
        "print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {len(df.columns)}\")\n",
        "\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "umap-bench",
      "language": "python",
      "name": "umap-bench"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
