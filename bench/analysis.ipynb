{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results Analysis\n",
    "\n",
    "This notebook loads all JSON files under `bench/results` and summarizes the benchmark runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f2fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316305f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 results files\n"
     ]
    }
   ],
   "source": [
    "results_dir = Path.cwd() / \"results\"\n",
    "json_files = sorted(results_dir.glob(\"bench-runs-*.json\"))\n",
    "print(f\"Found {len(json_files)} results files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae029a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runs: 60\n",
      "Rows: 60\n"
     ]
    }
   ],
   "source": [
    "runs = []\n",
    "rows = []\n",
    "\n",
    "for path in json_files:\n",
    "    payload = json.loads(path.read_text())\n",
    "    file_id = path.stem\n",
    "    for result in payload.get(\"results\", []):\n",
    "        runs.append({\n",
    "            \"file\": file_id,\n",
    "            \"generatedAt\": payload.get(\"generatedAt\"),\n",
    "            \"run\": result.get(\"run\"),\n",
    "            \"success\": result.get(\"success\"),\n",
    "            \"exitCode\": result.get(\"exitCode\"),\n",
    "            \"durationMs\": result.get(\"durationMs\"),\n",
    "            \"status\": result.get(\"status\"),\n",
    "        })\n",
    "        for block in result.get(\"uiMetrics\", []) or []:\n",
    "            for row in block.get(\"rows\", []) or []:\n",
    "                rows.append({\n",
    "                    \"file\": file_id,\n",
    "                    \"scenario\": row.get(\"scenario\", \"\"),\n",
    "                    \"datasetLabel\": row.get(\"datasetLabel\", \"\"),\n",
    "                    \"run\": row.get(\"run\"),\n",
    "                    \"runtimeMs\": row.get(\"runtimeMs\"),\n",
    "                    \"memoryMb\": row.get(\"memoryMb\"),\n",
    "                    \"qualityPercent\": row.get(\"qualityPercent\"),\n",
    "                    \"fps\": row.get(\"fps\"),\n",
    "                    \"latencyMs\": row.get(\"latencyMs\"),\n",
    "                    \"wasmFeatures\": row.get(\"wasmFeatures\", \"\"),\n",
    "                    \"dataset\": row.get(\"dataset\", \"\"),\n",
    "                })\n",
    "\n",
    "runs_df = pd.DataFrame(runs)\n",
    "rows_df = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"Runs: {len(runs_df)}\")\n",
    "print(f\"Rows: {len(rows_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success\n",
       "True    60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run-level overview\n",
    "runs_df.groupby(\"success\").size().rename(\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769062118389</th>\n",
       "      <td>10</td>\n",
       "      <td>351645.1</td>\n",
       "      <td>266674</td>\n",
       "      <td>1052903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769057694118</th>\n",
       "      <td>10</td>\n",
       "      <td>269170.0</td>\n",
       "      <td>265702</td>\n",
       "      <td>271671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769061162918</th>\n",
       "      <td>10</td>\n",
       "      <td>95453.8</td>\n",
       "      <td>92374</td>\n",
       "      <td>102112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769060386879</th>\n",
       "      <td>10</td>\n",
       "      <td>77519.0</td>\n",
       "      <td>74951</td>\n",
       "      <td>82843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769057053231</th>\n",
       "      <td>10</td>\n",
       "      <td>64012.0</td>\n",
       "      <td>60010</td>\n",
       "      <td>72456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bench-runs-1769056507010</th>\n",
       "      <td>10</td>\n",
       "      <td>54548.0</td>\n",
       "      <td>51961</td>\n",
       "      <td>58115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count      mean     min      max\n",
       "file                                                      \n",
       "bench-runs-1769062118389     10  351645.1  266674  1052903\n",
       "bench-runs-1769057694118     10  269170.0  265702   271671\n",
       "bench-runs-1769061162918     10   95453.8   92374   102112\n",
       "bench-runs-1769060386879     10   77519.0   74951    82843\n",
       "bench-runs-1769057053231     10   64012.0   60010    72456\n",
       "bench-runs-1769056507010     10   54548.0   51961    58115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duration by file\n",
    "runs_df.groupby(\"file\")[\"durationMs\"].agg([\"count\", \"mean\", \"min\", \"max\"]).sort_values(\"mean\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>wasmFeatures</th>\n",
       "      <th>datasetLabel</th>\n",
       "      <th>runs</th>\n",
       "      <th>runtime_ms_mean</th>\n",
       "      <th>runtime_ms_p95</th>\n",
       "      <th>memory_mb_mean</th>\n",
       "      <th>quality_mean</th>\n",
       "      <th>fps_mean</th>\n",
       "      <th>latency_ms_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4374.833333</td>\n",
       "      <td>7519.055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scenario wasmFeatures datasetLabel  runs  runtime_ms_mean  runtime_ms_p95  \\\n",
       "0                                        0      4374.833333        7519.055   \n",
       "\n",
       "  memory_mb_mean quality_mean fps_mean latency_ms_mean  \n",
       "0            NaN          NaN      NaN             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scenario-level summary\n",
    "summary = (\n",
    "    rows_df.groupby([\"scenario\", \"wasmFeatures\", \"datasetLabel\"], dropna=False)\n",
    "    .agg(\n",
    "        runs=(\"run\", \"count\"),\n",
    "        runtime_ms_mean=(\"runtimeMs\", \"mean\"),\n",
    "        runtime_ms_p95=(\"runtimeMs\", lambda s: s.quantile(0.95)),\n",
    "        memory_mb_mean=(\"memoryMb\", \"mean\"),\n",
    "        quality_mean=(\"qualityPercent\", \"mean\"),\n",
    "        fps_mean=(\"fps\", \"mean\"),\n",
    "        latency_ms_mean=(\"latencyMs\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"runtime_ms_mean\", ascending=False)\n",
    ")\n",
    "summary.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wasmFeatures</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenario</th>\n",
       "      <th>datasetLabel</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4374.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "wasmFeatures                      \n",
       "scenario datasetLabel             \n",
       "                       4374.833333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick comparison: WASM vs JS on the same scenario/dataset\n",
    "compare = (\n",
    "    rows_df.groupby([\"scenario\", \"datasetLabel\", \"wasmFeatures\"], dropna=False)\n",
    "    .agg(runtime_ms_mean=(\"runtimeMs\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "compare.pivot_table(\n",
    "    index=[\"scenario\", \"datasetLabel\"],\n",
    "    columns=\"wasmFeatures\",\n",
    "    values=\"runtime_ms_mean\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (umap-bench)",
   "language": "python",
   "name": "umap-bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
