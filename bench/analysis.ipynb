{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UMAP WebAssembly vs JavaScript Performance Analysis\n",
        "Performance-only analysis from benchmark JSON logs. Focus metrics: runtimeMs, memoryDeltaMb, fpsAvg, responsivenessMs.\n",
        "Trustworthiness is used only as a sanity check (no correctness analysis).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'figure.figsize': (10, 4),\n",
        "    'axes.titlesize': 12,\n",
        "    'axes.labelsize': 10,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9,\n",
        "})\n",
        "\n",
        "DATA_FILES = [\n",
        "    Path('/mnt/data/bench-runs-1769593958138.json'),\n",
        "    Path('/mnt/data/bench-runs-1769594451146.json'),\n",
        "    Path('/mnt/data/bench-runs-1769595026443.json'),\n",
        "    Path('/mnt/data/bench-runs-1769597840823.json'),\n",
        "    Path('/mnt/data/bench-runs-1769598240434.json'),\n",
        "    Path('/mnt/data/bench-runs-1769598749010.json'),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "records = []\n",
        "results_records = []\n",
        "file_meta_records = []\n",
        "\n",
        "for path in DATA_FILES:\n",
        "    if not path.exists():\n",
        "        print(f'Missing file: {path}')\n",
        "        continue\n",
        "    with path.open('r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    file_wasmFeatures = data.get('wasmFeatures')\n",
        "    machine = data.get('machine') or {}\n",
        "    git = data.get('git') or {}\n",
        "\n",
        "    file_meta_records.append({\n",
        "        'source_file': path.name,\n",
        "        'file_wasmFeatures': file_wasmFeatures,\n",
        "        'machine_cpuModel': machine.get('cpuModel'),\n",
        "        'machine_cpuCores': machine.get('cpuCores'),\n",
        "        'machine_totalMemBytes': machine.get('totalMemBytes'),\n",
        "        'git_commit': git.get('commit'),\n",
        "        'git_branch': git.get('branch'),\n",
        "        'git_statusDirty': git.get('statusDirty'),\n",
        "    })\n",
        "\n",
        "    results = data.get('results') or []\n",
        "    for r_idx, result in enumerate(results):\n",
        "        result_exitCode = result.get('exitCode')\n",
        "        result_durationMs = result.get('durationMs')\n",
        "        resultLabel = result.get('resultLabel')\n",
        "        result_success = (result_exitCode == 0) if result_exitCode is not None else None\n",
        "\n",
        "        results_records.append({\n",
        "            'source_file': path.name,\n",
        "            'file_wasmFeatures': file_wasmFeatures,\n",
        "            'result_index': r_idx,\n",
        "            'result_exitCode': result_exitCode,\n",
        "            'result_success': result_success,\n",
        "            'result_durationMs': result_durationMs,\n",
        "            'resultLabel': resultLabel,\n",
        "        })\n",
        "\n",
        "        ui_metrics = result.get('uiMetrics') or []\n",
        "        for ui in ui_metrics:\n",
        "            rows = ui.get('rows') or []\n",
        "            for row in rows:\n",
        "                if not isinstance(row, dict):\n",
        "                    continue\n",
        "                rec = dict(row)\n",
        "                rec.update({\n",
        "                    'source_file': path.name,\n",
        "                    'file_wasmFeatures': file_wasmFeatures,\n",
        "                    'result_exitCode': result_exitCode,\n",
        "                    'result_success': result_success,\n",
        "                    'result_durationMs': result_durationMs,\n",
        "                    'resultLabel': resultLabel,\n",
        "                    'machine_cpuModel': machine.get('cpuModel'),\n",
        "                    'machine_cpuCores': machine.get('cpuCores'),\n",
        "                    'machine_totalMemBytes': machine.get('totalMemBytes'),\n",
        "                    'git_commit': git.get('commit'),\n",
        "                    'git_branch': git.get('branch'),\n",
        "                    'git_statusDirty': git.get('statusDirty'),\n",
        "                })\n",
        "                records.append(rec)\n",
        "\n",
        "golden_df = pd.DataFrame(records)\n",
        "results_df = pd.DataFrame(results_records)\n",
        "file_meta_df = pd.DataFrame(file_meta_records).drop_duplicates()\n",
        "\n",
        "numeric_cols = [\n",
        "    'runtimeMs', 'memoryDeltaMb', 'fpsAvg', 'responsivenessMs', 'trustworthiness',\n",
        "    'datasetSize', 'dimensions', 'machine_cpuCores', 'machine_totalMemBytes'\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    if col in golden_df.columns:\n",
        "        golden_df[col] = pd.to_numeric(golden_df[col], errors='coerce')\n",
        "\n",
        "if 'runtimeMs' not in golden_df.columns:\n",
        "    raise ValueError('runtimeMs column missing from flattened rows')\n",
        "golden_df = golden_df[golden_df['runtimeMs'] > 0].copy()\n",
        "\n",
        "def infer_mode(row):\n",
        "    file_feat = str(row.get('file_wasmFeatures') or '').lower()\n",
        "    wm = row.get('wasmMode')\n",
        "    wm_l = wm.lower() if isinstance(wm, str) else ''\n",
        "\n",
        "    if file_feat == 'none':\n",
        "        return 'js'\n",
        "    if file_feat == 'all':\n",
        "        return 'wasm'\n",
        "    if 'wasm:' in wm_l:\n",
        "        return 'wasm'\n",
        "    if 'js' in wm_l and 'wasm' not in wm_l:\n",
        "        return 'js'\n",
        "    return np.nan\n",
        "\n",
        "golden_df['mode'] = golden_df.apply(infer_mode, axis=1)\n",
        "\n",
        "dataset_base = golden_df.get('datasetName', pd.Series(['unknown'] * len(golden_df)))\n",
        "dataset_base = dataset_base.fillna('unknown').astype(str)\n",
        "if 'scope' in golden_df.columns:\n",
        "    scope_col = golden_df['scope']\n",
        "    scope_ok = scope_col.notna() & (scope_col.astype(str) != '') & (scope_col.astype(str) != 'nan')\n",
        "    golden_df['datasetLabel'] = np.where(scope_ok, dataset_base + ' | ' + scope_col.astype(str), dataset_base)\n",
        "else:\n",
        "    golden_df['datasetLabel'] = dataset_base\n",
        "\n",
        "golden_df.to_csv('/mnt/data/bench_golden_rows.csv', index=False)\n",
        "\n",
        "group_cols = ['datasetName', 'scope', 'mode']\n",
        "runtime_stats = (\n",
        "    golden_df\n",
        "    .groupby(group_cols)['runtimeMs']\n",
        "    .agg(['mean', 'median', 'std', 'count'])\n",
        "    .reset_index()\n",
        "    .rename(columns={'count': 'n'})\n",
        ")\n",
        "\n",
        "runtime_wide = runtime_stats.pivot_table(index=['datasetName', 'scope'], columns='mode')\n",
        "if not runtime_wide.empty:\n",
        "    runtime_wide.columns = [f'{stat}_{mode}' for stat, mode in runtime_wide.columns]\n",
        "    runtime_summary = runtime_wide.reset_index()\n",
        "else:\n",
        "    runtime_summary = pd.DataFrame(columns=['datasetName', 'scope'])\n",
        "\n",
        "def calc_speedup(row):\n",
        "    med_js = row.get('median_js')\n",
        "    med_wasm = row.get('median_wasm')\n",
        "    mean_js = row.get('mean_js')\n",
        "    mean_wasm = row.get('mean_wasm')\n",
        "    if pd.notna(med_js) and pd.notna(med_wasm) and med_wasm > 0:\n",
        "        return med_js / med_wasm\n",
        "    if pd.notna(mean_js) and pd.notna(mean_wasm) and mean_wasm > 0:\n",
        "        return mean_js / mean_wasm\n",
        "    return np.nan\n",
        "\n",
        "if not runtime_summary.empty:\n",
        "    scope_series = runtime_summary.get('scope')\n",
        "    base = runtime_summary['datasetName'].fillna('unknown').astype(str)\n",
        "    if scope_series is not None:\n",
        "        scope_ok = scope_series.notna() & (scope_series.astype(str) != '') & (scope_series.astype(str) != 'nan')\n",
        "        runtime_summary['datasetLabel'] = np.where(scope_ok, base + ' | ' + scope_series.astype(str), base)\n",
        "    else:\n",
        "        runtime_summary['datasetLabel'] = base\n",
        "    runtime_summary['speedup'] = runtime_summary.apply(calc_speedup, axis=1)\n",
        "\n",
        "runtime_summary.to_csv('/mnt/data/bench_runtime_summary.csv', index=False)\n",
        "\n",
        "print('Golden rows:', len(golden_df))\n",
        "print('Runtime summary rows:', len(runtime_summary))\n",
        "print('Saved: /mnt/data/bench_golden_rows.csv and /mnt/data/bench_runtime_summary.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A. Data overview\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Counts by mode:')\n",
        "display(golden_df['mode'].value_counts(dropna=False))\n",
        "\n",
        "if 'scope' in golden_df.columns:\n",
        "    print('Counts by scope (top 20):')\n",
        "    display(golden_df['scope'].value_counts(dropna=False).head(20))\n",
        "\n",
        "print('Counts by datasetName (top 20):')\n",
        "display(golden_df['datasetName'].value_counts(dropna=False).head(20))\n",
        "\n",
        "print('Machine and git metadata by file:')\n",
        "display(file_meta_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. Sanity checks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "dataset_counts = golden_df['datasetName'].value_counts()\n",
        "rep_datasets = dataset_counts.index.tolist()[:2] if len(dataset_counts) > 2 else dataset_counts.index.tolist()\n",
        "\n",
        "rep_df = golden_df[golden_df['datasetName'].isin(rep_datasets) & golden_df['mode'].isin(['js', 'wasm'])].copy()\n",
        "if 'timestamp' in rep_df.columns:\n",
        "    rep_df = rep_df.sort_values('timestamp')\n",
        "rep_df['run_index'] = rep_df.groupby(['datasetName', 'mode']).cumcount()\n",
        "\n",
        "if rep_datasets:\n",
        "    fig, axes = plt.subplots(1, len(rep_datasets), figsize=(6 * len(rep_datasets), 4), sharey=False)\n",
        "    if len(rep_datasets) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, ds in zip(axes, rep_datasets):\n",
        "        sub = rep_df[rep_df['datasetName'] == ds]\n",
        "        for mode in ['js', 'wasm']:\n",
        "            msub = sub[sub['mode'] == mode]\n",
        "            ax.plot(msub['run_index'], msub['runtimeMs'], marker='o', linestyle='-', label=mode)\n",
        "        ax.set_title(f'Runtime vs run index: {ds}')\n",
        "        ax.set_xlabel('Run index')\n",
        "        ax.set_ylabel('runtimeMs')\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "outlier_mask = pd.Series(False, index=golden_df.index)\n",
        "for (ds, mode), group in golden_df[golden_df['mode'].isin(['js', 'wasm'])].groupby(['datasetName', 'mode']):\n",
        "    if len(group) < 4:\n",
        "        continue\n",
        "    q1 = group['runtimeMs'].quantile(0.25)\n",
        "    q3 = group['runtimeMs'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    if iqr == 0 or pd.isna(iqr):\n",
        "        continue\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    mask = (group['runtimeMs'] < lower) | (group['runtimeMs'] > upper)\n",
        "    outlier_mask.loc[group.index] = mask\n",
        "\n",
        "outliers = golden_df[outlier_mask]\n",
        "print(f'Outliers by IQR: {len(outliers)} rows out of {len(golden_df)}')\n",
        "display(outliers[['datasetName', 'scope', 'mode', 'runtimeMs', 'datasetSize']].head(10))\n",
        "\n",
        "if 'trustworthiness' in golden_df.columns:\n",
        "    print('Trustworthiness summary (sanity check only):')\n",
        "    display(golden_df['trustworthiness'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Core runtime comparison (primary)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_summary = runtime_summary.copy()\n",
        "plot_summary = plot_summary.sort_values('datasetLabel') if not plot_summary.empty else plot_summary\n",
        "\n",
        "if not plot_summary.empty:\n",
        "    x = np.arange(len(plot_summary))\n",
        "    width = 0.35\n",
        "    mean_js = plot_summary['mean_js'] if 'mean_js' in plot_summary.columns else np.full(len(plot_summary), np.nan)\n",
        "    mean_wasm = plot_summary['mean_wasm'] if 'mean_wasm' in plot_summary.columns else np.full(len(plot_summary), np.nan)\n",
        "    fig, ax = plt.subplots(figsize=(max(8, len(plot_summary) * 1.2), 4))\n",
        "    ax.bar(x - width/2, mean_js, width, label='js')\n",
        "    ax.bar(x + width/2, mean_wasm, width, label='wasm')\n",
        "    ax.set_title('Mean runtimeMs by dataset (js vs wasm)')\n",
        "    ax.set_ylabel('Mean runtimeMs')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(plot_summary['datasetLabel'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_df = golden_df[golden_df['mode'].isin(['js', 'wasm']) & golden_df['datasetLabel'].notna()]\n",
        "datasets = sorted(plot_df['datasetLabel'].unique().tolist())\n",
        "if datasets:\n",
        "    data = []\n",
        "    positions = []\n",
        "    centers = []\n",
        "    for i, ds in enumerate(datasets):\n",
        "        for j, mode in enumerate(['js', 'wasm']):\n",
        "            vals = plot_df[(plot_df['datasetLabel'] == ds) & (plot_df['mode'] == mode)]['runtimeMs'].dropna().values\n",
        "            data.append(vals)\n",
        "            positions.append(i * 3 + j)\n",
        "        centers.append(i * 3 + 0.5)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(8, len(datasets) * 1.2), 5))\n",
        "    bp = ax.boxplot(data, positions=positions, widths=0.6, patch_artist=True, showfliers=False)\n",
        "    for i, box in enumerate(bp['boxes']):\n",
        "        box.set_facecolor('C0' if i % 2 == 0 else 'C1')\n",
        "        box.set_alpha(0.6)\n",
        "    ax.set_title('Runtime distribution by dataset (js vs wasm)')\n",
        "    ax.set_ylabel('runtimeMs')\n",
        "    ax.set_xticks(centers)\n",
        "    ax.set_xticklabels(datasets, rotation=45, ha='right')\n",
        "    ax.legend([\n",
        "        plt.Line2D([0], [0], color='C0', lw=6),\n",
        "        plt.Line2D([0], [0], color='C1', lw=6)\n",
        "    ], ['js', 'wasm'])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if 'datasetSize' in golden_df.columns:\n",
        "    agg = golden_df[(golden_df['mode'].isin(['js', 'wasm'])) & (golden_df['datasetSize'] > 0)]\n",
        "    agg = agg.groupby(['datasetSize', 'mode'])['runtimeMs'].median().reset_index()\n",
        "    if not agg.empty:\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        for mode in ['js', 'wasm']:\n",
        "            sub = agg[agg['mode'] == mode].sort_values('datasetSize')\n",
        "            ax.plot(sub['datasetSize'], sub['runtimeMs'], marker='o', label=mode)\n",
        "        ax.set_title('Median runtime vs datasetSize (log-x)')\n",
        "        ax.set_xlabel('datasetSize')\n",
        "        ax.set_ylabel('runtimeMs')\n",
        "        ax.set_xscale('log')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('datasetSize column not available for runtime vs size plot.')\n",
        "\n",
        "print('Runtime summary (mean/median/std/n) by datasetName + scope + mode:')\n",
        "display(runtime_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Scaling behaviour\n",
        "Log-log plots and simple linear regression on log(runtimeMs) ~ log(datasetSize) per mode.\n",
        "Interpretation is cautious: slope ~1 suggests roughly linear scaling, >1 super-linear, <1 sub-linear.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if 'datasetSize' in golden_df.columns:\n",
        "    scale_df = golden_df[(golden_df['mode'].isin(['js', 'wasm'])) & (golden_df['datasetSize'] > 0) & (golden_df['runtimeMs'] > 0)]\n",
        "\n",
        "    if not scale_df.empty:\n",
        "        fig, ax = plt.subplots(figsize=(7, 4))\n",
        "        for mode in ['js', 'wasm']:\n",
        "            sub = scale_df[scale_df['mode'] == mode]\n",
        "            ax.scatter(sub['datasetSize'], sub['runtimeMs'], alpha=0.4, label=mode)\n",
        "        ax.set_title('Runtime vs datasetSize (log-log)')\n",
        "        ax.set_xlabel('datasetSize')\n",
        "        ax.set_ylabel('runtimeMs')\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_yscale('log')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    scaling_slopes = {}\n",
        "    for mode in ['js', 'wasm']:\n",
        "        sub = scale_df[scale_df['mode'] == mode]\n",
        "        if len(sub) >= 2:\n",
        "            x = np.log10(sub['datasetSize'])\n",
        "            y = np.log10(sub['runtimeMs'])\n",
        "            slope, intercept = np.polyfit(x, y, 1)\n",
        "            scaling_slopes[mode] = slope\n",
        "\n",
        "    if scaling_slopes:\n",
        "        print('Estimated log-log slopes (runtime vs datasetSize):')\n",
        "        for mode, slope in scaling_slopes.items():\n",
        "            print(f'  {mode}: {slope:.3f}')\n",
        "    else:\n",
        "        print('Insufficient data for slope estimates.')\n",
        "else:\n",
        "    scaling_slopes = {}\n",
        "    print('datasetSize column not available for scaling analysis.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. Memory behaviour (secondary)\n",
        "Negative memory deltas can occur due to GC or allocator behavior; interpret direction cautiously.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if 'memoryDeltaMb' in golden_df.columns:\n",
        "    mem_df = golden_df[golden_df['mode'].isin(['js', 'wasm'])]\n",
        "    mem_stats = (\n",
        "        mem_df.groupby(['datasetLabel', 'mode'])['memoryDeltaMb']\n",
        "        .agg(['mean', 'median', 'std', 'count'])\n",
        "        .reset_index()\n",
        "    )\n",
        "    display(mem_stats)\n",
        "\n",
        "    mem_wide = mem_stats.pivot_table(index='datasetLabel', columns='mode', values='mean')\n",
        "    if not mem_wide.empty:\n",
        "        labels = mem_wide.index.tolist()\n",
        "        mean_js = mem_wide['js'] if 'js' in mem_wide.columns else np.full(len(labels), np.nan)\n",
        "        mean_wasm = mem_wide['wasm'] if 'wasm' in mem_wide.columns else np.full(len(labels), np.nan)\n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.35\n",
        "        fig, ax = plt.subplots(figsize=(max(8, len(labels) * 1.2), 4))\n",
        "        ax.bar(x - width/2, mean_js, width, label='js')\n",
        "        ax.bar(x + width/2, mean_wasm, width, label='wasm')\n",
        "        ax.set_title('Mean memoryDeltaMb by dataset (js vs wasm)')\n",
        "        ax.set_ylabel('Mean memoryDeltaMb')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('memoryDeltaMb column not available in data.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. Responsiveness & FPS (supporting)\n",
        "Responsiveness thresholds: <16ms ~ smooth, >50ms noticeable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if 'fpsAvg' in golden_df.columns:\n",
        "    fps_df = golden_df[(golden_df['mode'].isin(['js', 'wasm'])) & (golden_df['fpsAvg'] > 0)]\n",
        "    fps_stats = fps_df.groupby(['datasetLabel', 'mode'])['fpsAvg'].mean().reset_index()\n",
        "    fps_wide = fps_stats.pivot_table(index='datasetLabel', columns='mode', values='fpsAvg')\n",
        "    if not fps_wide.empty:\n",
        "        labels = fps_wide.index.tolist()\n",
        "        mean_js = fps_wide['js'] if 'js' in fps_wide.columns else np.full(len(labels), np.nan)\n",
        "        mean_wasm = fps_wide['wasm'] if 'wasm' in fps_wide.columns else np.full(len(labels), np.nan)\n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.35\n",
        "        fig, ax = plt.subplots(figsize=(max(8, len(labels) * 1.2), 4))\n",
        "        ax.bar(x - width/2, mean_js, width, label='js')\n",
        "        ax.bar(x + width/2, mean_wasm, width, label='wasm')\n",
        "        ax.set_title('Mean fpsAvg by dataset (js vs wasm)')\n",
        "        ax.set_ylabel('fpsAvg')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('fpsAvg column not available in data.')\n",
        "\n",
        "if 'responsivenessMs' in golden_df.columns:\n",
        "    resp_df = golden_df[(golden_df['mode'].isin(['js', 'wasm'])) & (golden_df['responsivenessMs'] > 0)]\n",
        "    resp_stats = resp_df.groupby(['datasetLabel', 'mode'])['responsivenessMs'].median().reset_index()\n",
        "    resp_wide = resp_stats.pivot_table(index='datasetLabel', columns='mode', values='responsivenessMs')\n",
        "    if not resp_wide.empty:\n",
        "        labels = resp_wide.index.tolist()\n",
        "        median_js = resp_wide['js'] if 'js' in resp_wide.columns else np.full(len(labels), np.nan)\n",
        "        median_wasm = resp_wide['wasm'] if 'wasm' in resp_wide.columns else np.full(len(labels), np.nan)\n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.35\n",
        "        fig, ax = plt.subplots(figsize=(max(8, len(labels) * 1.2), 4))\n",
        "        ax.bar(x - width/2, median_js, width, label='js')\n",
        "        ax.bar(x + width/2, median_wasm, width, label='wasm')\n",
        "        ax.axhline(16, color='green', linestyle='--', linewidth=1, label='16ms (smooth)')\n",
        "        ax.axhline(50, color='red', linestyle='--', linewidth=1, label='50ms (noticeable)')\n",
        "        ax.set_title('Median responsivenessMs by dataset (js vs wasm)')\n",
        "        ax.set_ylabel('responsivenessMs')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('responsivenessMs column not available in data.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## G. Confidence intervals (lightweight)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def bootstrap_ci(data, n_resamples=1000, ci=95, seed=0):\n",
        "    data = np.asarray(data)\n",
        "    if len(data) == 0:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    rng = np.random.default_rng(seed)\n",
        "    means = np.empty(n_resamples)\n",
        "    for i in range(n_resamples):\n",
        "        sample = rng.choice(data, size=len(data), replace=True)\n",
        "        means[i] = sample.mean()\n",
        "    low = np.percentile(means, (100 - ci) / 2)\n",
        "    high = np.percentile(means, 100 - (100 - ci) / 2)\n",
        "    return data.mean(), low, high\n",
        "\n",
        "ci_records = []\n",
        "for (ds_label, mode), group in golden_df[golden_df['mode'].isin(['js', 'wasm'])].groupby(['datasetLabel', 'mode']):\n",
        "    mean, low, high = bootstrap_ci(group['runtimeMs'].dropna().values)\n",
        "    ci_records.append({\n",
        "        'datasetLabel': ds_label,\n",
        "        'mode': mode,\n",
        "        'mean': mean,\n",
        "        'ci_low': low,\n",
        "        'ci_high': high,\n",
        "        'n': len(group)\n",
        "    })\n",
        "\n",
        "ci_df = pd.DataFrame(ci_records)\n",
        "ci_wide = ci_df.pivot_table(index='datasetLabel', columns='mode', values=['mean', 'ci_low', 'ci_high'])\n",
        "if not ci_wide.empty:\n",
        "    ci_wide.columns = [f'{stat}_{mode}' for stat, mode in ci_wide.columns]\n",
        "    ci_wide['speedup'] = ci_wide['mean_js'] / ci_wide['mean_wasm']\n",
        "    display(ci_wide.reset_index())\n",
        "else:\n",
        "    print('No CI data available (insufficient rows).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## H. Failure/timeout analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not results_df.empty:\n",
        "    results_df['failure'] = results_df['result_exitCode'].apply(lambda x: (x is not None) and (x != 0))\n",
        "    failure_summary = (\n",
        "        results_df\n",
        "        .groupby('file_wasmFeatures')\n",
        "        .agg(\n",
        "            total=('result_exitCode', 'size'),\n",
        "            failures=('failure', 'sum'),\n",
        "            missing_exitCode=('result_exitCode', lambda s: s.isna().sum())\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    failure_summary['failure_rate'] = failure_summary['failures'] / failure_summary['total']\n",
        "    display(failure_summary)\n",
        "else:\n",
        "    failure_summary = pd.DataFrame()\n",
        "    print('No results-level data found for failure analysis.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I. Thesis-ready summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary_lines = []\n",
        "\n",
        "if not runtime_summary.empty:\n",
        "    for _, row in runtime_summary.sort_values('datasetLabel').iterrows():\n",
        "        sp = row.get('speedup')\n",
        "        if pd.notna(sp):\n",
        "            summary_lines.append(f\"- {row['datasetLabel']}: {sp:.2f}x median-based speedup (js/wasm)\")\n",
        "        else:\n",
        "            summary_lines.append(f\"- {row['datasetLabel']}: speedup n/a (missing mode)\")\n",
        "\n",
        "overall_median_speedup = runtime_summary['speedup'].median() if 'speedup' in runtime_summary.columns else np.nan\n",
        "if pd.notna(overall_median_speedup):\n",
        "    summary_lines.append(f\"- Overall median speedup: {overall_median_speedup:.2f}x\")\n",
        "\n",
        "if scaling_slopes:\n",
        "    slope_parts = [f\"{mode} slope {slope:.2f}\" for mode, slope in scaling_slopes.items()]\n",
        "    summary_lines.append(\"- Scaling (log-log): \" + \", \".join(slope_parts) + \". Interpret cautiously.\")\n",
        "\n",
        "if 'memoryDeltaMb' in golden_df.columns:\n",
        "    mem_overall = golden_df[golden_df['mode'].isin(['js', 'wasm'])].groupby('mode')['memoryDeltaMb'].mean()\n",
        "    if not mem_overall.empty:\n",
        "        summary_lines.append(\n",
        "            f\"- Mean memoryDeltaMb: js={mem_overall.get('js', np.nan):.2f}, wasm={mem_overall.get('wasm', np.nan):.2f} (negatives can reflect GC)\"\n",
        "        )\n",
        "\n",
        "if 'responsivenessMs' in golden_df.columns:\n",
        "    resp_overall = (\n",
        "        golden_df[(golden_df['mode'].isin(['js', 'wasm'])) & (golden_df['responsivenessMs'] > 0)]\n",
        "        .groupby('mode')['responsivenessMs'].median()\n",
        "    )\n",
        "    if not resp_overall.empty:\n",
        "        summary_lines.append(\n",
        "            f\"- Median responsivenessMs: js={resp_overall.get('js', np.nan):.1f}, wasm={resp_overall.get('wasm', np.nan):.1f} (16ms smooth, 50ms noticeable)\"\n",
        "        )\n",
        "\n",
        "if 'failure_summary' in globals() and not failure_summary.empty:\n",
        "    total_failures = failure_summary['failures'].sum()\n",
        "    total_runs = failure_summary['total'].sum()\n",
        "    if total_runs > 0:\n",
        "        summary_lines.append(\n",
        "            f\"- Failures/timeouts: {int(total_failures)} of {int(total_runs)} runs ({total_failures/total_runs:.1%})\"\n",
        "        )\n",
        "\n",
        "display(Markdown(\"\\n\".join(summary_lines)))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}